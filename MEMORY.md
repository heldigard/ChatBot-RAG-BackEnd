# üß† MEMORY - ChatBot RAG Backend

**√öltima actualizaci√≥n:** 2025-11-23
**Estado:** ‚úÖ C√≥digo base funcional (FastAPI + RAG local con FAISS y Sentence-Transformers)
**Arquitectura:** FastAPI + RAG local (FAISS + Sentence Transformers) con LLM compatible OpenAI (openai package)

---

## üìã RESUMEN EJECUTIVO

Este backend **FastAPI** implementa un sistema RAG local (FAISS + Sentence Transformers) que crea respuestas basadas en archivos PDF. El LLM se conecta mediante un cliente compatible con OpenAI (`openai`) y `OPENAI_API_BASE` permite apuntar a diferentes proveedores (OpenAI, DeepSeek, Azure, etc.).

**Puntos clave:**
- RAG local: FAISS + SentenceTransformers por defecto
- Soporte opcional para embeddings remotos (OpenAI/OpenRouter) usando variables de entorno
- El sistema indexa PDFs desde `PDF_DIRECTORY` y persiste el vector store en `vector_store.*` en disco

---

## üèóÔ∏è ARQUITECTURA IMPLEMENTADA

### Patrones de Dise√±o
- **API Gateway/Proxy:** El backend act√∫a como intermediario
- **Stateless:** No mantiene estado entre peticiones
- **Separaci√≥n de responsabilidades:** Frontend ‚Üî Backend ‚Üî RAG local (FAISS) / LLM externo

### Flujo de Datos
```
Frontend (React) ‚Üí Backend (FastAPI) ‚Üí RAG local (FAISS) ‚Üí LLM (OpenAI-compatible)
```
 
---

## üìÅ ESTRUCTURA DEL PROYECTO

```
ChatBot-RAG-BackEnd/
‚îú‚îÄ‚îÄ app.py                 # Aplicaci√≥n FastAPI
‚îú‚îÄ‚îÄ pdf_processor.py       # Procesamiento de PDFs
‚îú‚îÄ‚îÄ embedding_manager.py   # FAISS + sentence-transformers o OpenAI embeddings
‚îú‚îÄ‚îÄ rag_system.py          # Orquestador RAG (build, retrieve, format)
‚îú‚îÄ‚îÄ llm_manager.py         # Cliente OpenAI compatible para generaci√≥n
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias
‚îú‚îÄ‚îÄ .env.example           # Variables de entorno ejemplo
‚îú‚îÄ‚îÄ static/                # Interfaz web est√°tica
‚îî‚îÄ‚îÄ pdfs/                  # PDFs a indexar
```

---

## üîß COMPONENTES IMPLEMENTADOS

### 1. **FastAPI App** (`app.py`)
**Estado:** ‚úÖ RAG local (FAISS) y LLM compatible con OpenAI funcionando

**Endpoints:**
- `GET /health` - Verificaci√≥n de estado del servicio
- `GET /stats` - Estad√≠sticas del RAG y vector store
- `POST /chat` - Endpoint principal de chat (question + conversation_history opcional)
- `POST /upload_pdf` - Subida de PDF e indexaci√≥n incremental
- `POST /rebuild` - Reconstrucci√≥n del vector store
- `GET /retrieve` - Endpoint de depuraci√≥n para recuperar fragmentos
- `GET /` - Sirve UI web moderna y responsive en `static/index.html` con dise√±o optimizado

**Caracter√≠sticas implementadas:**

- ‚úÖ RAG local (FAISS) y embeddings con `SentenceTransformers` por defecto
- ‚úÖ Soporte para embeddings remotos con `openai` si `USE_OPENAI_EMBEDDINGS=true`
- ‚úÖ Subida de PDFs y a√±adido incremental a vector store (`/upload_pdf`)
- ‚úÖ Modelo de datos `ChatRequest` con `question` y opcional `conversation_history`
- ‚úÖ Manejo robusto de errores con logging configurables
- ‚úÖ Configuraci√≥n CORS para desarrollo
- ‚úÖ Cliente reutilizable (singleton pattern)
- ‚úÖ Interfaz web moderna con dise√±o mejorado, alta legibilidad y jerarqu√≠a visual optimizada
- ‚úÖ Sistema de temas claro/oscuro con contraste mejorado para accesibilidad
- ‚úÖ Dise√±o responsive adaptado para dispositivos m√≥viles y escritorio

**Flujo de procesamiento:**
```python
1. El cliente env√≠a `POST /chat` con `question` y opcionalmente `conversation_history`.
2. El backend recupera documentos relevantes a trav√©s de `RAGSystem.retrieve_documents()`.
3. Se formatea el contexto con `RAGSystem.format_context()` (respectando `MAX_CONTEXT_CHARS`).
4. Se invoca a `LLMManager.generate_response()` con la pregunta y el contexto.
5. Se retornan `answer`, `sources`, `context_used`, y `metadata`.
```

**Mejoras vs versi√≥n anterior:**
- ‚úÖ RAG local con indexaci√≥n y recuperaci√≥n de documentos (FAISS) para respuestas basadas en PDFs
- ‚úÖ Opci√≥n de usar embeddings locales (sentence-transformers) o remotos (OpenAI/OpenRouter)
- ‚úÖ Inicio de servicios (RAG y LLM) en `startup` para reducir latencia en la primera petici√≥n

 
### 2. **Dependencias** (`requirements.txt`)

**Estado:** ‚úÖ Estables y enfocadas en RAG local y compatibilidad OpenAI
```text
# Core Framework
fastapi==0.109.0
uvicorn==0.27.0
python-multipart==0.0.6
python-dotenv==1.0.1

# PDF Processing
PyPDF2==3.0.1
pdfplumber==0.10.3

# Text Processing & Splitting
langchain==0.1.0
langchain-text-splitters==0.0.1
tiktoken>=0.5.2

# Vector Database
faiss-cpu==1.12.0
chromadb==0.4.22

# OpenAI Compatible API
openai==1.6.1

# Text Processing
sentence-transformers==2.2.2

# Utilities
numpy>=1.24.3
requests==2.32.3
```

 
### 3. **Variables de Entorno** (`.env.example`)
**Estado:** ‚úÖ Configuraci√≥n orientada a OpenAI / OpenRouter / DeepSeek y RAG local
```text
# OpenAI / OpenAI-compatible LLM
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Embeddings
USE_OPENAI_EMBEDDINGS=false
EMBEDDING_MODEL=all-MiniLM-L6-v2
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
OPENAI_EMBEDDING_API_BASE=
OPENAI_EMBEDDING_API_KEY=

# RAG
PDF_DIRECTORY=./pdfs
VECTOR_STORE_PATH=./vector_store
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Logging
LOG_LEVEL=INFO
```

 
### 4. **Despliegue** (`startup.sh`)
**Estado:** ‚úÖ Preparado para ejecuci√≥n local y despliegue en plataformas (Azure/AWS/GCP/Kubernetes)
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

---

## üéØ CUMPLIMIENTO DE REQUISITOS

 
### ‚úÖ Estado: RAG local implementado y funcional
- **Backend FastAPI:** ‚úÖ Implementado
- **Endpoints `/health` y `/chat`:** ‚úÖ Funcionales
-- **Llamadas HTTP a LLM OpenAI-compatible:** ‚úÖ Operativas (configurable mediante `OPENAI_API_BASE`)
- **Gesti√≥n de errores:** ‚úÖ Robusta
- **Variables de entorno:** ‚úÖ Configuradas
- **Script de inicio Azure:** ‚úÖ Preparado

 
### ‚ö†Ô∏è DESAF√çO CALDAS - 70% CUMPLIDO
**‚úÖ Cumple:**
- Asistente legal funcional
- Respuestas basadas en documentos legales
- Interfaz chat implementada (en frontend)
- Sistema end-to-end operativo

**‚ùå No cumple (por dise√±o del plan):**
- No tests unitarios completos ni CI
- Falta rate limiting y control de producci√≥n (rate limits/CORS)

---

## üîç ESTADO DE CALIDAD

### ‚úÖ **Fortalezas**
1. **C√≥digo limpio** y bien estructurado
2. **Manejo robusto de errores** con HTTPException
3. **Logging implementado** para debugging
4. **Documentaci√≥n completa** en README
5. **Configuraci√≥n segura** con .env.example
6. **Listo para producci√≥n** en Azure

### ‚ö†Ô∏è **√Åreas de Mejora**

#### Cr√≠ticas (Para cumplir Desaf√≠o Caldas)
1. **A√±adir endpoint `/upload_pdf`:** Permitir carga din√°mica de PDFs
2. **Implementar RAG local:** Chroma/FAISS + embeddings
3. **Procesamiento de PDFs:** Extracci√≥n y chunking de texto

#### Sugeridas (Buenas pr√°cticas)
1. **Testing unitario:** tests/test_app.py
2. **Rate limiting:** slowapi para prevenir abuse
3. **CORS producci√≥n:** Restringir or√≠genes espec√≠ficos
4. **Logging estructurado:** JSON format para producci√≥n
5. **Dockerfile:** Para contenerizaci√≥n

---

## üîß TAREAS PENDIENTES

### **High Priority**
1. A√±adir tests unitarios e integraci√≥n (CI) para endpoints y flujos RAG (upload -> rebuild -> chat).
2. Implementar rate limiting y protecci√≥n (por ejemplo `slowapi`).
3. Hacer `upload_pdf` idempotente y documentar comportamiento incremental de indexaci√≥n.
4. A√±adir m√©tricas y alarmas (Prometheus/Datadog) para monitorizar la salud del servicio.

### **Medium Priority (Mejoras)**
1. Implementar streaming SSE/websockets para respuestas si el LLM lo soporta.
2. Contenerizar la app (Dockerfile) y a√±adir pipelines de CI/CD.
3. Mejorar logs y observabilidad (JSON structured logging, request traces).

### **Low Priority (Opcional)**
```bash
# 1. Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

# 2. Estructura mejorada
mkdir -p src/api src/models src/services
```

---

## üß™ TESTING - ESTADO ACTUAL

**Estado:** ‚úÖ Tests b√°sicos existen (`tests/test_basic_endpoints.py`) ‚Äî requiere mayor cobertura e integraci√≥n CI

**Tests sugeridos:**
```python
# tests/test_app.py
import pytest
from fastapi.testclient import TestClient
from app import app

client = TestClient(app)

def test_health():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}

def test_chat_empty_question():
    response = client.post("/chat", json={"question": ""})
    assert response.status_code == 400

def test_chat_valid_question():
    # Se espera que haga un POST a /chat y retorne 'answer'.
    response = client.post("/chat", json={"question": "¬øQu√© es la Ley 1257?"})
    assert response.status_code in (200, 500)
    assert isinstance(response.json(), dict)

**Nota (Postman):** La colecci√≥n `postman/` fue actualizada para eliminar los requests dependientes de `threads` (p. ej. `threads/create`). Use los endpoints `Chat (No thread)` y `Chat - Follow-up (no thread)` para pruebas r√°pidas.
```

---

## üìä M√âTRICAS Y MONITOREO

**Estado:** ‚ö†Ô∏è Logging b√°sico implementado

**Mejoras sugeridas:**
```python
# logging estructurado
import structlog
logger = structlog.get_logger()

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    logger.info("chat_request_received", question=request.question)
    # ... l√≥gica existente
    logger.info("chat_response_sent", response_length=len(answer))
```

---

## üöÄ DESPLIEGUE

### Despliegue (recomendaciones)
- **Runtime:** Python 3.11
- **Startup Command:** `uvicorn app:app --host 0.0.0.0 --port 8000`
- **Variables de entorno:** Configurar en la plataforma de despliegue (Azure/AWS/K8s) y rotar claves.
- **CORS:** Restringir a dominios del frontend en producci√≥n.

---

## üîó INTEGRACIONES

### Frontend
- **URL Base:** `import.meta.env.VITE_API_URL || "http://localhost:8000"`
- **Endpoint Chat:** `POST /chat`
- **Request Format:** `{"question": "texto pregunta"}`
- **Response Format:** `{"answer": "respuesta", "sources": ["fuente1", "fuente2"]}`

### OpenAI / OpenAI-compatible LLM
- **Formato:** API compat. OpenAI (Chat Completions)
- **Headers:** `{'Content-Type': 'application/json', 'Authorization': 'Bearer <OPENAI_API_KEY>'}` o `api_key` si el proveedor lo requiere
- **Timeout:** Depende del proveedor, configurar en `OPENAI_API_BASE` en caso de proveedores alternativos

---

## üìù DECISIONES DE DISE√ëO IMPORTANTES

1. **RAG local por defecto:** FAISS + SentenceTransformers para reproducibilidad y reducci√≥n de costos.
2. **No persistencia de conversaciones por defecto:** `conversation_history` se pasa por request para que el cliente controle el contexto.
3. **Se prioriza una arquitectura modular:** `PDFProcessor`, `EmbeddingManager`, `RAGSystem`, `LLMManager`.
4. **Configuraci√≥n mediante `.env`:** Facilita cambiar proveedores (OpenAI, DeepSeek, Azure, LocalAI) en entornos distintos.
5. **CORS permisivo en desarrollo:** Debe restringirse en producci√≥n.

---

## üö® PROBLEMAS CONOCIDOS Y TAREAS PENDIENTES

### ‚úÖ Completado/Corregido
1. `.gitignore` creado para proteger secretos y elementos generados.
2. C√≥digo estructurado con manejo de errores y logs b√°sico.

### ‚ö†Ô∏è Pendientes (Prioridad)
1. **Testing:** Aumentar la cobertura de unidades e integraci√≥n; a√±adir CI.
2. **Rate limiting / protecci√≥n:** Implementar `slowapi` o middleware equivalente.
3. **CORS producci√≥n:** Restringir a or√≠genes permitidos.
4. **Backups sincronizados:** Mantener versiones y backups del `vector_store`.

---

## üîÑ ESTADO DE DESARROLLO

**Desarrollo:** ‚úÖ Implementado (RAG local + LLM)
**Testing:** ‚ö†Ô∏è Cobertura inicial (pruebas de endpoints), ampliar con unitarias e integraci√≥n
**Documentaci√≥n:** ‚úÖ README y Memory actualizados
**Despliegue:** ‚úÖ Compatible con Azure/AWS/GCP/Docker
**Calidad:** ‚ö†Ô∏è Mejorar con tests y observabilidad

---

## üìû CONTACTO Y SOPORTE

**Para cambios o mejoras:**
1. Revisar este archivo `MEMORY.md` primero
2. Verificar `README.md` y `postman/` para pruebas r√°pidas
3. Asegurarse de que `OPENAI_API_KEY` y otros secretos no sean versionados

**Pr√≥ximos desarrolladores:**
- Mantener `MEMORY.md` sincronizado con cambios en endpoints y capacidades RAG
- A√±adir notas de migraci√≥n si se integra con proveedores gestionados (Azure, etc.)

---

**üéØ NOTA FINAL:** Este repositorio implementa un orquestador RAG local (FAISS + Sentence-Transformers) con un LLM compatible con la API de OpenAI. Para producci√≥n, recomendamos a√±adir rate limiting, auditor√≠a/logging y pruebas de integraci√≥n.

---

## üÜï CAMBIOS RECIENTES

### Cambios implementados (Resumen)

**Antes:** Proyecto con un prototipo de orquestador y collection en Postman.

**Ahora:**
- Implementado RAG local con `FAISS` y embeddings (`sentence-transformers`) por defecto.
- A√±adido endpoint `/upload_pdf` para indexado incremental.
- Implementado `LLMManager` con `openai` (compatible con m√∫ltiples `OPENAI_API_BASE`).
- Implementado `rebuild` y `retrieve` endpoints para flujo RAG y debug.
- **MEJORA CR√çTICA:** Implementado SystemPrompt personalizado para evitar respuestas JSON y asegurar formato texto plano.

**Archivos modificados/manuales:**
1. `app.py` - Endpoints y orquestaci√≥n de RAG + carga de SystemPrompt
2. `requirements.txt` - Dependencias para RAG local
3. `pdf_processor.py`, `embedding_manager.py`, `rag_system.py`, `llm_manager.py` - Core RAG pieces
4. `SystemPrompt.txt` - Prompt optimizado para respuestas en texto plano con formato legal
5. `static/index.html` - Interfaz web completamente redise√±ada con UX/UI moderna
6. `postman/` - Collection de pruebas (revisar si hay endpoints no implementados)

### üéØ Cambio m√°s importante: SystemPrompt optimizado

**Problema resuelto:** El sistema generaba respuestas en formato JSON como:
```json
{
    "answer": "La Ley 1257 de 2008 tiene como objetivo...",
    "sources": [...],
    "metadata": {...}
}
```

**Soluci√≥n implementada:**
- Se cre√≥ `SystemPrompt.txt` con instrucciones expl√≠citas para generar respuestas en texto plano
- Se modific√≥ `llm_manager.py` para cargar el SystemPrompt autom√°ticamente
- Se actualiz√≥ `app.py` para pasar el SystemPrompt al LLM en cada solicitud

**Resultado esperado:** Respuestas en formato texto plano legible:
```
La Ley 1257 de 2008 tiene como objetivo la adopci√≥n de normas que garanticen para todas las mujeres una vida libre de violencia. [Fuente: Ley 1257 de 2008, Art√≠culo 1]

Esta ley busca asegurar el ejercicio de los derechos de las mujeres... [Fuente: Ley 1257 de 2008, Art√≠culo 2]
```

### üé® Cambio reciente: Interfaz de Usuario Modernizada

**Problema resuelto:** La interfaz web ten√≠a problemas de saturaci√≥n de color y mala legibilidad debido al dise√±o con gradientes morados intensos y bajo contraste entre elementos.

**Soluci√≥n implementada:**

- Redise√±o completo del sistema de colores con paleta moderna y accesible
- Mejora dr√°stica del contraste y jerarqu√≠a visual
- Implementaci√≥n de sistema de temas claro/oscuro optimizado
- Dise√±o responsive para m√≥viles y escritorio

**Mejoras principales:**

1. **Esquema de color optimizado:**
   - Fondo: Gradiente suave `#f0f9ff ‚Üí #e0e7ff ‚Üí #f5f3ff` (modo claro)
   - Contenedores: Blancos con alta opacidad (95%) para m√°xima legibilidad
   - Acentos: Azules modernos `#6366f1` con mejor contraste

2. **Jerarqu√≠a visual mejorada:**
   - Separaci√≥n clara entre header, chat, input y footer
   - Bordes y sombras optimizados para profundidad
   - Mensajes del usuario: Color s√≥lido con buen contraste
   - Mensajes del bot: Fondos blancos con sombras sutiles

3. **Accesibilidad y UX:**
   - Sistema de temas claro/oscuro con transiciones suaves
   - Estados interactivos mejorados (hover, focus)
   - Indicadores de carga y conexi√≥n m√°s visibles
   - Fuentes de documento interactivas con mejor feedback visual

4. **Responsive design:**
   - Adaptaci√≥n perfecta a m√≥viles, tablets y escritorio
   - Componentes flexibles que se redimensionan correctamente
   - Controles t√°ctiles optimizados para dispositivos m√≥viles

**Resultado:** Una interfaz moderna, profesional y altamente usable que cumple con est√°ndares de accesibilidad y proporciona una experiencia de usuario superior.
